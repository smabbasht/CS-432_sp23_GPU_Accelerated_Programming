{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HelloWorldGPU.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smabbasht/CS-432_sp23_GPU_Accelerated_Programming/blob/main/Week1/HelloWorldGPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBUHmmqQHOS-",
        "outputId": "8b844435-880e-4f03-981c-4383b0829c55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-3q_uzwts\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-3q_uzwts\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit aac710a35f52bb78ab34d2e52517237941399eff\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-py3-none-any.whl size=4304 sha256=3e638b34ed0e8767f313e7b6294b710ae85b13db718eb4570e51d47b4c2088fc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-grmig3mp/wheels/f3/08/cc/e2b5b0e1c92df07dbb50a6f024a68ce090f5e7b2316b41756d\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aikKbs6TMEp6",
        "outputId": "e268e248-fd78-482e-cce5-47c251ebb0c9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jan 13 07:09:48 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8    12W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void HelloKernel() {\n",
        "    printf(\"\\tHello from GPU (device)\\n\");\n",
        "}\n",
        "\n",
        "int main() {\n",
        "  printf(\"Hello from CPU (host) before kernel execution\\n\");\n",
        "  HelloKernel<<<1,32>>>();\n",
        "  cudaDeviceSynchronize();\n",
        "  printf(\"Hello from CPU (host) after kernel execution\\n\");\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P71PB1boHhgz",
        "outputId": "9e79c759-2371-425c-f2ee-8abed34f7b2f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from CPU (host) before kernel execution\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "\tHello from GPU (device)\n",
            "Hello from CPU (host) after kernel execution\n",
            "\n"
          ]
        }
      ]
    }
  ]
}